{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from fake_useragent import UserAgent\n",
    "\n",
    "# Define user agents\n",
    "ua = UserAgent()\n",
    "headers = {\n",
    "    'User-Agent': ua.random\n",
    "}\n",
    "\n",
    "# Define a function to retry requests\n",
    "def fetch_url(url, retries=5):\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers)\n",
    "            response.raise_for_status()\n",
    "            return response\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error fetching {url}: {e}. Retrying... ({i+1}/{retries})\")\n",
    "            time.sleep(random.uniform(1, 3))\n",
    "    return None\n",
    "\n",
    "# Load scraped links from a file\n",
    "def load_scraped_links(filename):\n",
    "    try:\n",
    "        with open(filename, 'r', encoding='utf-8') as file:\n",
    "            return json.load(file)\n",
    "    except (FileNotFoundError, json.JSONDecodeError):\n",
    "        return []\n",
    "\n",
    "# Save scraped links to a file\n",
    "def save_scraped_link(link, filename):\n",
    "    scraped_links = load_scraped_links(filename)\n",
    "    if link not in scraped_links:\n",
    "        scraped_links.append(link)\n",
    "        with open(filename, 'w', encoding='utf-8') as file:\n",
    "            json.dump(scraped_links, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Append article to the JSON file\n",
    "def append_article(article, category):\n",
    "    filename_articles = f\"scraped_articles_{category}.json\"\n",
    "    articles = load_scraped_links(filename_articles)\n",
    "    articles.append(article)\n",
    "    with open(filename_articles, 'w', encoding='utf-8') as file:\n",
    "        json.dump(articles[-5:], file, ensure_ascii=False, indent=4)  # Keep only last 5 articles\n",
    "\n",
    "# Scrape article content\n",
    "def scrape_article_content(article, category):\n",
    "    response = fetch_url(article['href'])\n",
    "    if not response:\n",
    "        article['content'] = ['Failed to retrieve content']\n",
    "    else:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        content_div = soup.find('div', class_='ArticleBodyCont')\n",
    "        if content_div:\n",
    "            paragraphs = content_div.find_all('p')\n",
    "            content = [re.sub(r'http\\S+', '', p.text.strip()) for p in paragraphs if p.text.strip()]\n",
    "            article['content'] = content\n",
    "        else:\n",
    "            article['content'] = ['Content not available']\n",
    "    append_article(article, category)\n",
    "\n",
    "# Scrape page and articles\n",
    "def scrape_page(url, scraped_links_filename, category):\n",
    "    scraped_links = load_scraped_links(scraped_links_filename)\n",
    "    response = fetch_url(url)\n",
    "    if not response:\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    figure_blocks = soup.find_all('figure')\n",
    "    articles = []\n",
    "\n",
    "    for block in figure_blocks[:5]:  # Limit to top 5 articles\n",
    "        a_tag = block.find('a', href=True)\n",
    "        if a_tag and a_tag['href'] not in scraped_links:\n",
    "            href = a_tag['href']\n",
    "            card_title_div = a_tag.find('div', class_='card_title')\n",
    "            if card_title_div:\n",
    "                title_tag = card_title_div.find('h3')\n",
    "                if title_tag:\n",
    "                    title = title_tag.text.strip()\n",
    "                    article = {'title': title, 'href': href, 'category': category}\n",
    "                    articles.append(article)\n",
    "                    save_scraped_link(href, scraped_links_filename)\n",
    "                    scrape_article_content(article, category)\n",
    "        if len(articles) >= 5:  # Stop after collecting 5 articles\n",
    "            break\n",
    "    return articles\n",
    "\n",
    "# Scrape all pages for a category\n",
    "def scrape_all_pages(base_url, scraped_links_filename, category):\n",
    "    page = 1\n",
    "    all_articles = []\n",
    "    while len(all_articles) < 5:\n",
    "        url = f\"{base_url}/page/{page}/\"\n",
    "        articles = scrape_page(url, scraped_links_filename, category)\n",
    "        if not articles:\n",
    "            break\n",
    "        all_articles.extend(articles)\n",
    "        page += 1\n",
    "    return all_articles[:5]\n",
    "\n",
    "# Scrape multiple categories concurrently\n",
    "def scrape_categories(categories):\n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        futures = []\n",
    "        for category, base_url in categories.items():\n",
    "            scraped_links_filename = f\"scraped_links_{category}.json\"\n",
    "            articles_filename = f\"scraped_articles_{category}.json\"\n",
    "\n",
    "            # Ensure files exist\n",
    "            if not os.path.exists(scraped_links_filename):\n",
    "                with open(scraped_links_filename, 'w', encoding='utf-8') as file:\n",
    "                    json.dump([], file)\n",
    "\n",
    "            if not os.path.exists(articles_filename):\n",
    "                with open(articles_filename, 'w', encoding='utf-8') as file:\n",
    "                    json.dump([], file)\n",
    "\n",
    "            futures.append(executor.submit(scrape_all_pages, base_url, scraped_links_filename, category))\n",
    "\n",
    "        for future in futures:\n",
    "            future.result()\n",
    "\n",
    "# Define categories and their base URLs\n",
    "categories = {\n",
    "    'Entertainment': 'https://tv9telugu.com/entertainment',\n",
    "    'Andhra-Pradesh': 'https://tv9telugu.com/andhra-pradesh',\n",
    "    'Telangana': 'https://tv9telugu.com/telangana',\n",
    "    'Sports': 'https://tv9telugu.com/sports',\n",
    "    'national': 'https://tv9telugu.com/national',\n",
    "    'politics': 'https://tv9telugu.com/politics',\n",
    "    'Crime': 'https://tv9telugu.com/crime',\n",
    "    'health': 'https://tv9telugu.com/health',\n",
    "    'Business': 'https://tv9telugu.com/business',\n",
    "    'Lifestyle': 'https://tv9telugu.com/lifestyle',\n",
    "    'Technology': 'https://tv9telugu.com/technology',\n",
    "    'Spiritual': 'https://tv9telugu.com/spiritual',\n",
    "    'International': 'https://tv9telugu.com/world',\n",
    "}\n",
    "\n",
    "# Start scraping\n",
    "scrape_categories(categories)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vipplav",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
